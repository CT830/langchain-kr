from urllib import response
from requests import session
import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_teddynote.prompts import load_prompt
from langchain_openai import ChatOpenAI
from langchain_teddynote import logging
from langchain.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_openai import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os

# API KEY ì •ë³´ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("[Project] Multi Tuen ì±—ë´‡")


# ì´ì „ ëŒ€í™”ë¥¼ ì¶œë ¥
def print_messages():
    for chat_message in st.session_state["messages"]:
        st.chat_message(chat_message.role).write(chat_message.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))


# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_ids):
    if session_ids not in st.session_state["store"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ storeì— ì €ì¥
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜


def load_faiss_db(embeddings):
    db_path = "./faiss_1_row_index"
    faiss_db = FAISS.load_local(
        db_path, embeddings, allow_dangerous_deserialization=True
    )
    return faiss_db


def load_retriever():
    # ë‹¨ê³„ 1: ì„ë² ë”©(Embedding) ìƒì„±
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # ë‹¨ê³„ 2: DB ë¶ˆëŸ¬ì˜¤ê¸°
    vectorstore = load_faiss_db(embeddings)

    # ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
    # ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.
    retriever = vectorstore.as_retriever()
    return retriever


def format_doc(document_list):
    return "\n\n".join([doc.page_content for doc in document_list])


# ì²´ì¸ ìƒì„±
def create_chain(retriever, model_name="gpt-4o"):
    # my_prompt = load_prompt("prompts/my-rag.yaml", encoding="utf-8")
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are an AI assistant who is an entrepreneur and knows the relevant support programs."
                "Please answer the question in Korean based on the context you are given."
                "Here is the context you should refer to:: {context}",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            (
                "human",
                "Question\n:{question}",
            ),
        ]
    )

    # llm ìƒì„±
    llm = ChatOpenAI(model_name=model_name, temperature=0)

    # ì²´ì¸ êµ¬ì„±
    # retriever_chain = RunnableParallel(
    #     {"context": lambda _: load_retriever(), "question": RunnablePassthrough()}
    # )

    chain = (
        {"context": retriever | format_doc, "question": RunnablePassthrough()}
        # {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,  # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        input_messages_key="question",  # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ í…œí”Œë¦¿ ë³€ìˆ˜ì— ë“¤ì–´ê°ˆ key
        history_messages_key="chat_history",  # ê¸°ë¡ ë©”ì‹œì§€ì˜ í‚¤
    )
    return chain_with_history


# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

st.title("ëŒ€í™”ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” ì±—ë´‡ ğŸ’¬")

# ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ê¸° ìœ„í•œ ìš©ë„ë¡œ ìƒì„±í•œë‹¤.
    st.session_state["messages"] = []
    retriver = load_retriever()
    chain = create_chain(retriver, model_name="gpt-4o")
    st.session_state["chain"] = chain

if "store" not in st.session_state:
    st.session_state["store"] = {}


# ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")

    # ëª¨ë¸ ì„ íƒ ë©”ë‰´
    # selected_model = st.selectbox("LLM ì„ íƒ", ["gpt-4o", "gpt-4o-mini"], index=0)

    # ì„¸ì…˜ ID ë¥¼ ì§€ì •í•˜ëŠ” ë©”ë‰´
    session_id = st.text_input("ì„¸ì…˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "abc123")


# ì´ˆê¸°í™” ë²„íŠ¼ì´ ëˆŒë¦¬ë©´...
if clear_btn:
    st.session_state["messages"] = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ì‚¬ìš©ìì˜ ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

if "chain" not in st.session_state:
    retriver = load_retriever()
    st.session_state["chain"] = create_chain(retriver, model_name="gpt-4o")


# ë§Œì•½ì— ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´...
if user_input:
    chain = st.session_state["chain"]
    if chain is not None:
        try:
            response = chain.stream(
                # ì§ˆë¬¸ ì…ë ¥
                {"question": user_input},
                # ì„¸ì…˜ ID ê¸°ì¤€ìœ¼ë¡œ ëŒ€í™”ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.
                config={"configurable": {"session_id": session_id}},
            )

            # ì‚¬ìš©ìì˜ ì…ë ¥
            st.chat_message("user").write(user_input)

            with st.chat_message("assistant"):
                # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
                container = st.empty()

                ai_answer = ""
                for token in response:
                    ai_answer += token
                    container.markdown(ai_answer)

                # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
                add_message("user", user_input)
                add_message("assistant", ai_answer)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    else:
        st.error("ì²´ì¸ì´ ì´ˆê¸°í™” ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
